{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Building Simple CNN Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- docs: https://www.tensorflow.org/api_docs/python/tf/WholeFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 현재 디렉토리를 이용해서 파일 이름을 확인한다.\n",
    "# for dir in os.listdir(os.getcwd()):\n",
    "#     print dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 여러장의 이미지를 대상으로 학습을 진행한다.\n",
    "MULTI_IMAGE_MODE = True\n",
    "\n",
    "\n",
    "image_list = None\n",
    "label_list = None\n",
    "\n",
    "if MULTI_IMAGE_MODE:\n",
    "    batch_size = -1\n",
    "\n",
    "    image_width = 49\n",
    "    image_height = 61\n",
    "    image_channel = 1\n",
    "\n",
    "\n",
    "    image_dir = os.getcwd() + '/test_img/'\n",
    "    image_list = os.listdir(image_dir)\n",
    "    image_list.sort()\n",
    "\n",
    "    # print image_list[:10]\n",
    "\n",
    "    image_list = [image_dir + filename for filename in image_list]\n",
    "\n",
    "    # print image_list[:10]\n",
    "\n",
    "    label_dir = os.getcwd() + '/test_label/label.csv'\n",
    "    label_list = [label_dir]\n",
    "\n",
    "# 한두장의 이미지로 학습한다.\n",
    "else:\n",
    "    batch_size = -1\n",
    "\n",
    "    image_width = 100\n",
    "    image_height = 100\n",
    "    image_channel = 3\n",
    "\n",
    "    image_path1 = os.getcwd() + '/image.png'\n",
    "    image_path2 = os.getcwd() + '/image-grayscaled.png'\n",
    "    label_path1 = os.getcwd() + '/label1.csv'\n",
    "    label_path2 = os.getcwd() + '/label2.csv'\n",
    "\n",
    "    image_list = [image_path1, image_path2]\n",
    "    label_list = [label_path1, label_path2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "filename_queue = tf.train.string_input_producer(image_list)\n",
    "labelname_queue = tf.train.string_input_producer(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "image_reader = tf.WholeFileReader()\n",
    "label_reader = tf.TextLineReader()\n",
    "\n",
    "image_key, image_value = image_reader.read(filename_queue)\n",
    "label_key, label_value = label_reader.read(labelname_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "image_decoded = tf.image.decode_png( image_value )\n",
    "label_decoded = tf.decode_csv( label_value, record_defaults=[ [0] ])\n",
    "\n",
    "image_decoded = tf.reshape(image_decoded, [image_height, image_width, image_channel])\n",
    "label_decoded = tf.reshape(label_decoded, [1])\n",
    "\n",
    "image_batch, label_batch = tf.train.shuffle_batch( \n",
    "             [image_decoded, label_decoded],\n",
    "             batch_size=32, capacity=5000, min_after_dequeue=1000)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"shuffle_batch_1:0\", shape=(32, 61, 49, 1), dtype=uint8)\n",
      "Tensor(\"shuffle_batch_1:1\", shape=(32, 1), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print image_batch\n",
    "print label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 61, 49, 1) <type 'numpy.ndarray'>\n",
      "(32, 1) <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "    for i in range(10):\n",
    "        _image_batch, _label_batch = sess.run([image_batch, label_batch])\n",
    "\n",
    "    print _image_batch.shape, type(_image_batch)\n",
    "    print _label_batch.shape, type(_label_batch)\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'numpy.ndarray'>\n",
      "(32, 61, 49, 1)\n",
      "<type 'numpy.ndarray'>\n",
      "(32, 1)\n"
     ]
    }
   ],
   "source": [
    "# print _image_batch\n",
    "print type(_image_batch)\n",
    "print _image_batch.shape\n",
    "\n",
    "# print _label_batch\n",
    "print type(_label_batch)\n",
    "print _label_batch.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Encoding and Decoding : https://www.tensorflow.org/api_guides/python/image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Build Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Reference : https://www.tensorflow.org/get_started/mnist/pros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Placeholder - Input Data Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder( tf.float32, shape=[None, image_height, image_width, image_channel]  )\n",
    "y_ = tf.placeholder( tf.float32, shape=[None, 1] )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder_8:0\", shape=(?, 61, 49, 1), dtype=float32)\n",
      "Tensor(\"Placeholder_9:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print x\n",
    "print y_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Variables - Weight Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fc_width = 13\n",
    "fc_height = 16\n",
    "\n",
    "# Convolution Layer 1\n",
    "hidden1_w = tf.Variable( tf.truncated_normal([5, 5, image_channel, 32]) )\n",
    "hidden1_b = tf.Variable( tf.zeros([32]) )\n",
    "\n",
    "# Convolution Layer 2\n",
    "hidden2_w = tf.Variable(tf.truncated_normal([5, 5, 32, 64]))\n",
    "hidden2_b = tf.Variable(tf.zeros([64]))\n",
    "\n",
    "# Densely Connected Layer\n",
    "fully_connected_w = tf.Variable(tf.truncated_normal([fc_width*fc_height*64, 1024]))\n",
    "fully_connected_b = tf.Variable(tf.zeros([1024]))\n",
    "\n",
    "# Dropout\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Readout Layer\n",
    "output_w = tf.Variable(tf.truncated_normal([1024, 1]))\n",
    "output_b = tf.Variable(tf.zeros([1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MaxPool_37:0\", shape=(?, 16, 13, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Conv Layer 1\n",
    "x_image = tf.reshape(x, [-1, image_height, image_width,image_channel])\n",
    "y_label = tf.reshape(y_, [-1, 1])\n",
    "\n",
    "hidden1_layer = tf.nn.relu(tf.nn.conv2d(x_image, hidden1_w, strides=[1, 1, 1, 1], padding='SAME') + hidden1_b)\n",
    "hidden1_pool = tf.nn.max_pool(hidden1_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Conv Layer 2\n",
    "hidden2_layer = tf.nn.relu(tf.nn.conv2d(hidden1_pool, hidden2_w, strides=[1, 1, 1, 1], padding='SAME') + hidden2_b)\n",
    "hidden2_pool = tf.nn.max_pool(hidden2_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "print hidden2_pool\n",
    "\n",
    "# Densely(Fully) Connected Layer\n",
    "hidden_pool2_flat = tf.reshape(hidden2_pool, [-1, fc_width*fc_height*64])\n",
    "hidden_fc1 = tf.nn.relu(tf.matmul(hidden_pool2_flat, fully_connected_w) + fully_connected_b)\n",
    "\n",
    "# Dropout\n",
    "hidden_fc1_dropout = tf.nn.dropout(hidden_fc1, keep_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Convolution 네트워크는 종료되고 Feed Forward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#Readout Layer\n",
    "prediction = tf.matmul(hidden_fc1_dropout, output_w) + output_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate loss (Evaluate performance) and doing back propagation\n",
    "cross_entropy = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( labels=y_, logits= prediction ) )\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Preprocessing - Test for showing the _print_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  -7.08617e+07\n",
      "accuracy:  1.0\n",
      "Convolution Network Training is done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  -1.03629e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  -3.18231e+07\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  1.03337e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  2.53421e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  1.561e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  5.36719e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  4.24639e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  5.9918e+08\n",
      "accuracy:  1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "loss:  6.36268e+08\n",
      "accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "    # CNN학습을 진행한다.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(10):\n",
    "        _, _loss, _accuracy = sess.run([train_step, cross_entropy, accuracy], \n",
    "                                       feed_dict={x:_image_batch,\n",
    "                                                  y_:_label_batch,\n",
    "                                                  keep_prob: 0.5})\n",
    "        print \"----------\"\n",
    "        print \"loss: \", _loss\n",
    "        print \"accuracy: \", _accuracy\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join()\n",
    "    print 'Convolution Network Training is done'\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e916041124da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_print_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m             \u001b[0;31m# print(typekey)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136d0410>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-e916041124da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_print_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moriginal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/PIL/Image.pyc\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m             \u001b[0;31m# print(typekey)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2198\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot handle this data type\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2199\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2200\u001b[0m         \u001b[0mrawmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 61, 49, 1)\n",
      "(61, 49, 1)\n",
      "INFO:tensorflow:Error reported to Coordinator: <type 'exceptions.RuntimeError'>, Attempted to use a closed Session.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    coord = tf.train.Coordinator()\n",
    "    thread = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "    # 입력받은 이미지를 출력한다.\n",
    "    fig = plt.figure()\n",
    "\n",
    "    _print_images = sess.run( [image_batch] )\n",
    "    _print_images = np.array(_print_images, np.float32)\n",
    "    print _print_images.shape\n",
    "    print _print_images[0,0,:,:,:].shape\n",
    "\n",
    "    for i in range(5):\n",
    "        image = _print_images[0,i,:,:,:]\n",
    "        original = Image.fromarray(image.reshape[image_height, )\n",
    "\n",
    "        fig.add_subplot(5,1,i+1)\n",
    "        imgplot = plt.imshow(np.asarray(original))\n",
    "\n",
    "    coord.request_stop()\n",
    "    coord.join()\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- 컴퓨터의 벡터 연산과 수학에서 표현하는 벡터간의 표현차이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
      "(2, 5)\n",
      "[[1], [2], [3], [4], [5]]\n",
      "(5, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_in_computer = [[1,2,3,4,5], [6,7,8,9,10]]\n",
    "print vector_in_computer\n",
    "print np.shape(vector_in_computer)\n",
    "\n",
    "vector_in_math = [[1], [2], [3], [4], [5]]\n",
    "print vector_in_math\n",
    "print np.shape(vector_in_math)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  },
  "name": "TensorFlow-Convolution-Neural-Network-Practice.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
